
@misc{VaswaniAttentionAllYou2017a,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-12-05},
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2022-12-07},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}
@online{OpenAI22,
  author =       "OpenAI",
  title =        "GPT-3 models: text-davinci-002 and text-davinci-003",
  url =          "https://beta.openai.com/docs/api-reference/introduction",
  lastaccessed = "November 30, 2022",
}

@inproceedings{souza2020bertimbau,
  author    = {F{\'a}bio Souza and
               Rodrigo Nogueira and
               Roberto Lotufo},
  title     = {{BERT}imbau: pretrained {BERT} models for {B}razilian {P}ortuguese},
  booktitle = {9th Brazilian Conference on Intelligent Systems, {BRACIS}, Rio Grande do Sul, Brazil, October 20-23 (to appear)},
  year      = {2020}
}

@misc{https://doi.org/10.48550/arxiv.1907.05338,
  doi = {10.48550/ARXIV.1907.05338},
  url = {https://arxiv.org/abs/1907.05338},
  author = {Wang, Ran and Su, Haibo and Wang, Chunye and Ji, Kailin and Ding, Jupeng},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {To Tune or Not To Tune? How About the Best of Both Worlds?},
  publisher = {arXiv},
  year = {2019},
  copyright = {Creative Commons Zero v1.0 Universal}
}


@article{kuvcak2018machine,
  title={MACHINE LEARNING IN EDUCATION-A SURVEY OF CURRENT RESEARCH TRENDS.},
  author={Ku{\v{c}}ak, Danijel and Juri{\v{c}}i{\'c}, Vedran and {\DJ}ambi{\'c}, Goran},
  journal={Annals of DAAAM \& Proceedings},
  volume={29},
  year={2018}
}

@article{luan2021review,
  title={A review of using machine learning approaches for precision education},
  author={Luan, Hui and Tsai, Chin-Chung},
  journal={Educational Technology \& Society},
  volume={24},
  number={1},
  pages={250--266},
  year={2021},
  publisher={JSTOR}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@misc{https://doi.org/10.48550/arxiv.2005.14165,
  doi = {10.48550/ARXIV.2005.14165},
  
  url = {https://arxiv.org/abs/2005.14165},
  
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Language Models are Few-Shot Learners},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@INPROCEEDINGS {8575587,
author = {I. Cataneo Silveira and D. Deratani Maua},
booktitle = {2018 7th Brazilian Conference on Intelligent Systems (BRACIS)},
title = {Advances in Automatically Solving the ENEM},
year = {2018},
volume = {},
issn = {},
pages = {43-48},
abstract = {Answering questions formulated in natural language is a long standing quest in Artificial Intelligence. However, even formulating the problem in precise terms has proven to be too challenging, which lead many researchers to focus on Multiple-Choice Question Answering problems. One particularly interesting type of the latter problem is solving standardized tests such as university entrance exams. The Exame Nacional do Ensino MeÃÅdio (ENEM) is a High School level exam widely used by Brazilian universities as entrance exam, and the world's second biggest university entrance examination in number of registered candidates. In this work we tackle the problem of answering purely textual multiple-choice questions from the ENEM. We build on a previous solution that formulated the problem as a text information retrieval problem. In particular, we investigate how to enhance these methods by text augmentation using Word Embedding and WordNet, a structured lexical database where words are connected according to some relations like synonymy and hypernymy. We also investigate how to boost performance by building ensembles of weakly correlated solvers. Our approaches obtain accuracies ranging from 26% to 29.3%, outperforming the previous approach.},
keywords = {knowledge based systems;information retrieval;encyclopedias;electronic publishing;internet;mathematics},
doi = {10.1109/BRACIS.2018.00016},
url = {https://doi.ieeecomputersociety.org/10.1109/BRACIS.2018.00016},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {oct}
}

@article{DBLP:journals/corr/abs-2201-11903,
  author    = {Jason Wei and
               Xuezhi Wang and
               Dale Schuurmans and
               Maarten Bosma and
               Ed H. Chi and
               Quoc Le and
               Denny Zhou},
  title     = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  journal   = {CoRR},
  volume    = {abs/2201.11903},
  year      = {2022},
  url       = {https://arxiv.org/abs/2201.11903},
  eprinttype = {arXiv},
  eprint    = {2201.11903},
  timestamp = {Fri, 22 Apr 2022 16:06:31 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2201-11903.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{MikolovEfficientEstimationWord2013a,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  date = {2013-09-06},
  number = {arXiv:1301.3781},
  eprint = {1301.3781},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1301.3781},
  url = {http://arxiv.org/abs/1301.3781},
  urldate = {2022-09-21},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

% @article{raffel2019exploring,
%   title={Exploring the limits of transfer learning with a unified text-to-text transformer},
%   author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
%   journal={arXiv preprint arXiv:1910.10683},
%   year={2019}
% }